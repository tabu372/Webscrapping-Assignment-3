{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the required libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task no.1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first connect to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\HP\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "time.sleep(6)\n",
    "\n",
    "url = \"https://www.amazon.in/\"\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "\n",
    "# entering the data into Search bar\n",
    "search_field=driver.find_element_by_id(\"twotabsearchtextbox\") #product search bar\n",
    "search_field.send_keys(\"watches\")\n",
    "\n",
    "# clicking the search button\n",
    "search_button=driver.find_element_by_xpath(\"//*[@id='nav-search-submit-button']\")\n",
    "search_button.click()\n",
    "time.sleep(4)\n",
    "\n",
    "# creating empty lists for scraping data\n",
    "Brand_name=[]\n",
    "Price=[]\n",
    "Product_URL=[]\n",
    "Rating=[]\n",
    "Numb_of_rating=[]\n",
    "time.sleep(3)\n",
    "\n",
    "urls=[]\n",
    "#scrapping the required details\n",
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):\n",
    "    brands=driver.find_elements_by_class_name('s-line-clamp-1')\n",
    "    for i in brands:\n",
    "        Brand_name.append(i.text)\n",
    "#scrapping Price\n",
    "    prices=driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "    for i in prices:\n",
    "        Price.append(i.text)\n",
    "    nxt_button=driver.find_elements_by_xpath(\"//li[@class='a-last']\")\n",
    "    try:\n",
    "        driver.get(nxt_button[1].get_attribute('href'))\n",
    "    except:\n",
    "        driver.get(nxt_button[0].get_attribute('href'))\n",
    "\n",
    "time.sleep(2)\n",
    "for page in range(0,3):\n",
    "    product_url = driver.find_elements_by_xpath(\"//a[@class='a-link-normal a-text-normal']\")\n",
    "    for i in product_url:\n",
    "        Product_URL.append(i.get_attribute('href'))\n",
    "    nxt_button=driver.find_elements_by_xpath(\"//li[@class='a-last']\")\n",
    "    try:\n",
    "        driver.get(nxt_button[1].get_attribute('href'))\n",
    "    except:\n",
    "        driver.get(nxt_button[0].get_attribute('href'))\n",
    "        \n",
    "# scrapping rating of the product\n",
    "urls=driver.find_elements_by_xpath(\"//a[@class='a-link-normal a-text-normal']\")\n",
    "UR=[]\n",
    "for i in urls[0:7]:\n",
    "    UR.append(i.get_attribute('href'))\n",
    "for url in UR:\n",
    "    driver.get(url)\n",
    "    try:                                                                      \n",
    "        rating=driver.find_element_by_xpath(\"//span[@class='a-size-base a-nowrap']//span\")\n",
    "        Rating.append(rating.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Rating.append(\"NO rating\")\n",
    "        \n",
    "#number of rating\n",
    "urls=driver.find_elements_by_xpath(\"//a[@class='a-link-normal a-text-normal']\")\n",
    "UR=[]\n",
    "for i in urls[0:7]:\n",
    "    UR.append(i.get_attribute('href'))\n",
    "for url in UR:\n",
    "    driver.get(url)\n",
    "    try:                                                                      \n",
    "        ratings=driver.find_element_by_xpath(\"//span[@class='a-link-normal']\")\n",
    "        Numb_of_rating.append(rating.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Numb_of_rating.append(\"-\")\n",
    "        \n",
    "df=pd.DataFrame({\"Brand\":Brand_name,\"Rating\":Rating,\"Number_of_rating\":Numb_of_rating,\n",
    "                 \"Price\":Price,\"url\":Product_URL})\n",
    "#saving the data into csv file\n",
    "df.columns = ['Brand', 'Rating', 'Number_of_rating', 'Price', 'Product_URL']\n",
    "df.to_csv('Watch_details.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task no.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a python program to access the search bar and search button on images.google.com and \n",
    "scrape 100 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import os\n",
    "import time\n",
    "import urllib.request\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\HP\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver.get(\"https://www.google.com/\")\n",
    "search = driver.find_element_by_name('q')\n",
    "search.send_keys('fruits',Keys.ENTER)\n",
    "\n",
    "#entered the search and need to switch to images tab on the browse page to search for images.\n",
    "elem = driver.find_element_by_link_text('Images')\n",
    "elem.get_attribute('href')\n",
    "elem.click()\n",
    "\n",
    "#\n",
    "value = 0\n",
    "for i in range(20): \n",
    "    driver.execute_script('scrollBy(\"+ str(value) +\",+100);')\n",
    "    value += 100\n",
    "time.sleep(4)\n",
    "#\n",
    "elements = driver.find_elements_by_xpath('//img[contains(@class,\"rg_i\")]')\n",
    "#\n",
    "for i in elements:\n",
    "    src = i.get_attribute('src')\n",
    "    try:\n",
    "        if src != None:\n",
    "            src  = str(src)\n",
    "            count=1\n",
    "            urllib.request.urlretrieve(src, os.path.join('fruits','fruits'+str(count)+'.jpg'))\n",
    "        else:\n",
    "            raise TypeError\n",
    "    except TypeError:\n",
    "        pass\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task no.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on \n",
    "google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "import pandas as pd\n",
    "import time\n",
    "from openpyxl import load_workbook\n",
    "from random import randint\n",
    "\n",
    "wb_name = \"Book1.xlsx\" #file name\n",
    "\n",
    "\n",
    "wb = load_workbook(wb_name, data_only = True)\n",
    "ws = wb['sheet_name'']\n",
    "address_list =[]\n",
    "link_col = A1  #column number\n",
    "\n",
    "coord_prospects = pd.DataFrame() \n",
    "\n",
    "for row in ws.iter_rows(min_row = x , max_row = x, min_col = link_col, max_col=link_col):\n",
    "    if str(row[0].value) != \"None\":\n",
    "        address_list.append(row[0].value)\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\HP\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "#driver.minimize_window() #this is optional if the opening google chrome window gets annoying\n",
    "driver.get('https://www.mapdevelopers.com/geocode_tool.php')\n",
    "add = driver.find_element_by_class_name('form-control')\n",
    "\n",
    "for t,a in enumerate(address_list):\n",
    "    print (\"Geocoding...\",t+1,\"/\",len(address_list),str(round(t/len(address_list)*100,2)),\"%\",\" : \", a)\n",
    "    add.clear()\n",
    "    add.send_keys(a)\n",
    "    try:\n",
    "        search1 = driver.find_element_by_xpath('//*[@id=\"search-form\"]/div[1]/span[2]').click()\n",
    "        time.sleep(3)\n",
    "        search2 = driver.find_element_by_xpath('//*[@id=\"search-form\"]/div[1]/span[2]').click()\n",
    "        time.sleep(3)\n",
    "    except ElementClickInterceptedException:\n",
    "        time.sleep(2)\n",
    "        search = driver.find_element_by_xpath('//*[@id=\"search-form\"]/div[1]/span[2]').click()\n",
    "    lat=driver.find_element_by_id('display_lat')\n",
    "    lng=driver.find_element_by_id('display_lng')\n",
    "    street=driver.find_element_by_id('display_address')\n",
    "    city=driver.find_element_by_id('display_city')\n",
    "    postcode=driver.find_element_by_id('display_zip')\n",
    "    state=driver.find_element_by_id('display_state')\n",
    "    county=driver.find_element_by_id('display_county')\n",
    "    country=driver.find_element_by_id('display_country')\n",
    "    latlng = pd.DataFrame({'Latitude':pd.Series(lat.text),\n",
    "                            'Longitude':pd.Series(lng.text),\n",
    "                            'Street':pd.Series(street.text),\n",
    "                            'City':pd.Series(city.text),\n",
    "                            'Postcode':pd.Series(postcode.text),\n",
    "                            'State':pd.Series(state.text),\n",
    "                            'County':pd.Series(county.text),\n",
    "                            'Country':pd.Series(country.text)})\n",
    "    coord_prospects = coord_prospects.append(latlng, ignore_index=True)\n",
    "    print(coord_prospects.tail(1))\n",
    "    print(\"   \")\n",
    "\n",
    "coord_prospects.to_excel('Book1.xlsx') #name of output excel file\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task no.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a program to scrap all the available details of top 10 gaming laptops from digit.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first connect to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\HP\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "time.sleep(6)\n",
    "\n",
    "url = \"https://www.digit.in/\"\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "\n",
    "#clicking on search feild\n",
    "search_field=driver.find_element_by_class_name(\"search\").click()\n",
    "\n",
    "#Entering details on search keys\n",
    "search=driver.find_element_by_id(\"globalPageSearchText\")\n",
    "search.send_keys(\"gamming laptops\", Keys.ENTER)\n",
    "\n",
    "#creating the empty list\n",
    "product_name=[]\n",
    "description=[]\n",
    "price=[]\n",
    "review=[]\n",
    "page_urls = []\n",
    "\n",
    "# scrape next pages urls\n",
    "nxt_page = driver.find_elements_by_xpath(\"//li[@class='pagination-number']/a\")\n",
    "for i in nxt_page:\n",
    "    page_urls.append(i.get_attribute('href'))\n",
    "for url in page_urls[:3]:\n",
    "    driver.get(url)\n",
    "    Names=driver.find_elements_by_xpath(\"//div[@class='searchProduct-desc'])\n",
    "    for i in Names:\n",
    "        product_name.append(i.text)\n",
    "    \n",
    "    desc=driver.find_elements_by_xpath(\"//div[@class='MoreLink-briefdescription']\")#for scrapping description\n",
    "    for i in desc:\n",
    "        description.append(i.text)\n",
    "    \n",
    "    rs=driver.find_elements_by_xpath(\"//div[@class='product-price']\")  #for scrapping prices\n",
    "    for i in rs:\n",
    "        price.append(i.text)\n",
    "                                        \n",
    "    rv=driver.find_elements_by_xpath(\"//div[@class='UserDate greentext']\")#for scrapping review\n",
    "    for i in rv:\n",
    "        review.append(i.text)\n",
    "                                        \n",
    "df=pd.DataFrame({'Brand': product_name[:10],'description': description[:10],'Price': price[:10],'Review': review[:10]})\n",
    "df        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task no.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on \n",
    "www.flipkart.com and scrape following details for all the search results displayed on 1st page. \n",
    "Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, \n",
    "“Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Display \n",
    "Resolution”, “Processor”, “Processor Cores”, “Battery Capacity”, “Price”, “Product URL”. \n",
    "Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe \n",
    "and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "# creating empty list\n",
    "brand_name = []    \n",
    "RAM=[]\n",
    "Display=[]\n",
    "Camera=[]\n",
    "Battery=[]\n",
    "Processor=[]\n",
    "Accessories=[]\n",
    "Price=[]\n",
    "rating=[]\n",
    "\n",
    "pages= list(range(1,2))\n",
    "for page in pages:\n",
    "    req=requests.get(\"https://www.flipkart.com/search?q=smartphone&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page={}\".format(page)).text\n",
    "    #content of the page\n",
    "    soup=BeautifulSoup(req,'html.parser')\n",
    "    \n",
    "    brand=soup.find_all('div',class_='_4rR01T')\n",
    "    for i in range(len(brand)):\n",
    "        brand_name.append(brand[i].text)\n",
    "        len(brand_name)\n",
    "        \n",
    "    commonclass=soup.find_all('li',class_='rgWa7D')\n",
    "    #create empty list for the features\n",
    "    for i in range(0,len(commonclass)):\n",
    "        p=commonclass[i].text\n",
    "        if(\"RAM\" in p):\n",
    "            RAM.append(p)\n",
    "        elif(\"Display\" in p):\n",
    "            Display.append(p)\n",
    "        elif(\"Camera\" in p):\n",
    "            Camera.append(p)\n",
    "        elif(\"Battery\" in p):\n",
    "            Battery.append(p)\n",
    "        elif(\"Processor\" in p):\n",
    "            Processor.append(p)\n",
    "        elif(\"Accessories\" in p):\n",
    "            Accessories.append(p)\n",
    "        \n",
    "    Price=soup.find_all('div',class_='_30jeq3 _1_WHN1')\n",
    "    for i in range(len(Price)):\n",
    "        Price.append(Price[i].text)\n",
    "        len(Price) \n",
    "  \n",
    "    rating=soup.find_all('div',class_='_3LWZlK')\n",
    "    for i in range(len(rating)):\n",
    "        rating.append(rating[i].text)\n",
    "        len(rating)\n",
    "        \n",
    "#keeping into dataframe       \n",
    "df= {'brand_name':brand_name[40],'RAM':RAM[40],'Display':Display[slice(40)],'Camera':Camera[slice(40)],'Battery':Battery[slice(40)],'Processor':Processor[slice(40)],'Accessories':Accessories[slice(40)],'Price':Price[40],'rating':rating[40]}\n",
    "dataset=pd.DataFrame(data = df)\n",
    "\n",
    "#storing in csv file\n",
    "dataset.to_csv('smartphone.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task no.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a program to scrap details of all the funding deals for second quarter (i.e. July 20 –\n",
    "September 20) from trak.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
